{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db37d17d",
   "metadata": {},
   "source": [
    "# Minitest Comparisons With Ability Expectations \n",
    "\n",
    "This series of snippets conducts a number of analyses consisting on verifying if the sequence of eight questions that were used in the experiment -as a whole- for all students -as a group- fell within the range of expectations (>5% confidence). A series the datasets were used:\n",
    "1. In the case of the diagnostic, the 36-row and 8-column dataset with the results was built by selecting the eight questions out of the 21 that form the whole test, and 36 students out of the 87 that eventually participated in the study.  This dataset is called \"Diagnostic Answers\" (DA).\n",
    "2. The 40-row and 8-column dataset of the Preliminary Answers (PA) is made of the results of same eight questions by the 40 students. \n",
    "3. Likewise, the Final Answers (FA) dataset is also 40-row and 8-column dataset in the same fashion.\n",
    "4. Correctness probabilities dataset (CP). This contains, for each student, the probability of answering correctly each of the eight selected questions. \n",
    "\n",
    "The verifications are:\n",
    "1. Verifying if DA are within expectations predicted by CP\n",
    "2. Verifying if PA are within expectations predicted by CP\n",
    "3. Verifying if FA are within expectations predicted by CP.\n",
    "    * Verifying if DA (Non-Deception questions 1,2,4,5,6,8) are within expectations predicted by CP\n",
    "    * Verifying if DA (Deception questions 3 and 7) are within expectations predicted by CP\n",
    "\n",
    "\n",
    "This analysis concluded that verifications 1 and 2 are suitable, but not verification 3. The problem with this approach is that this is not an optimal measure to compare with the other two cases. Whereas for 1, 2, 4, 5, 6 and 8, students are being influenced towards correctness, for questions 3 and 7 students were swayed towarsd incorrectness. The optimal measure is one that combines how much the students deviated from expectations in both directions, instead of how much they deviated to correctness despite the results in Q3 an 7, which is the current case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d94ff0",
   "metadata": {},
   "source": [
    "# Diagnostic minitest (all questions)\n",
    "\n",
    "Here the algorithm to show that the results in the minitest are within the range of the expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "be9ca42b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean expected score: 2.7675\n",
      "Mean observed score: 3.6667\n",
      "Mean variance: 1.7594\n",
      "Z-score (mean comparison): 0.6779\n",
      "p-value (mean comparison): 0.2489\n",
      "95% Confidence Interval for Expected Mean: [0.1678, 5.3673]\n",
      "There is no significant difference between the mean of the observed and expected results (p >= 0.05).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Load CSV files\n",
    "prob_df = pd.read_csv('DiagnosticProbabilities_8q.csv', index_col=0)\n",
    "results_df = pd.read_csv('DiagnosticResults_36s_8q.csv', index_col=0)\n",
    "\n",
    "# Question values\n",
    "#values = np.array([0.110, 0.106, 0.095, 0.174, 0.114, 0.108, 0.096, 0.196])\n",
    "#values = np.array([0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125])\n",
    "values = np.array([1, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "# Calculate the expected score for each student\n",
    "expected_scores = (prob_df * values).sum(axis=1)\n",
    "\n",
    "# Calculate the expected variance for each student\n",
    "expected_variances = ((prob_df * (1 - prob_df)) * (values ** 2)).sum(axis=1)\n",
    "\n",
    "# Calculate the observed score for each student\n",
    "observed_scores = (results_df * values).sum(axis=1)\n",
    "\n",
    "# Calculate the mean expected score and mean variance\n",
    "mean_expected_score = expected_scores.mean()\n",
    "mean_variance = expected_variances.sum() / len(expected_scores)\n",
    "\n",
    "# Calculate the mean observed score\n",
    "mean_observed_score = observed_scores.mean()\n",
    "\n",
    "# Calculate the Z-score for the mean comparison\n",
    "z_score_mean = (mean_observed_score - mean_expected_score) / np.sqrt(mean_variance)\n",
    "\n",
    "# Calculate the p-value for the mean comparison\n",
    "p_value_mean = 1 - norm.cdf(z_score_mean)\n",
    "\n",
    "# Calculate the standard error\n",
    "standard_error = np.sqrt(mean_variance)\n",
    "\n",
    "# Calculate the 95% confidence interval for the expected mean\n",
    "ci_lower = mean_expected_score - 1.96 * standard_error\n",
    "ci_upper = mean_expected_score + 1.96 * standard_error\n",
    "\n",
    "print(f\"Mean expected score: {mean_expected_score:.4f}\")\n",
    "print(f\"Mean observed score: {mean_observed_score:.4f}\")\n",
    "print(f\"Mean variance: {mean_variance:.4f}\")\n",
    "print(f\"Z-score (mean comparison): {z_score_mean:.4f}\")\n",
    "print(f\"p-value (mean comparison): {p_value_mean:.4f}\")\n",
    "print(f\"95% Confidence Interval for Expected Mean: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "\n",
    "# Conclusion\n",
    "if p_value_mean < 0.05:\n",
    "    print(\"There is a significant difference between the mean of the observed and expected results (p < 0.05).\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the mean of the observed and expected results (p >= 0.05).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c1240dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean expected score: 0.3459\n",
      "Mean observed score: 0.4583\n",
      "Mean variance: 0.0275\n",
      "Z-score (mean comparison): 0.6779\n",
      "p-value (mean comparison): 0.2489\n",
      "95% Confidence Interval for Expected Mean: [0.0210, 0.6709]\n",
      "There is no significant difference between the mean of the observed and expected results (p >= 0.05).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Load CSV files\n",
    "prob_df = pd.read_csv('DiagnosticProbabilities_8q.csv', index_col=0)\n",
    "results_df = pd.read_csv('DiagnosticResults_36s_8q.csv', index_col=0)\n",
    "\n",
    "# Question values\n",
    "#values = np.array([0.110, 0.106, 0.095, 0.174, 0.114, 0.108, 0.096, 0.196])\n",
    "values = np.array([0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125])\n",
    "\n",
    "# Calculate the expected score for each student\n",
    "expected_scores = (prob_df * values).sum(axis=1)\n",
    "\n",
    "# Calculate the expected variance for each student\n",
    "expected_variances = ((prob_df * (1 - prob_df)) * (values ** 2)).sum(axis=1)\n",
    "\n",
    "# Calculate the observed score for each student\n",
    "observed_scores = (results_df * values).sum(axis=1)\n",
    "\n",
    "# Calculate the mean expected score and mean variance\n",
    "mean_expected_score = expected_scores.mean()\n",
    "mean_variance = expected_variances.sum() / len(expected_scores)\n",
    "\n",
    "# Calculate the mean observed score\n",
    "mean_observed_score = observed_scores.mean()\n",
    "\n",
    "# Calculate the Z-score for the mean comparison\n",
    "z_score_mean = (mean_observed_score - mean_expected_score) / np.sqrt(mean_variance)\n",
    "\n",
    "# Calculate the p-value for the mean comparison\n",
    "p_value_mean = 1 - norm.cdf(z_score_mean)\n",
    "\n",
    "# Calculate the standard error\n",
    "standard_error = np.sqrt(mean_variance)\n",
    "\n",
    "# Calculate the 95% confidence interval for the expected mean\n",
    "ci_lower = mean_expected_score - 1.96 * standard_error\n",
    "ci_upper = mean_expected_score + 1.96 * standard_error\n",
    "\n",
    "print(f\"Mean expected score: {mean_expected_score:.4f}\")\n",
    "print(f\"Mean observed score: {mean_observed_score:.4f}\")\n",
    "print(f\"Mean variance: {mean_variance:.4f}\")\n",
    "print(f\"Z-score (mean comparison): {z_score_mean:.4f}\")\n",
    "print(f\"p-value (mean comparison): {p_value_mean:.4f}\")\n",
    "print(f\"95% Confidence Interval for Expected Mean: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "\n",
    "# Conclusion\n",
    "if p_value_mean < 0.05:\n",
    "    print(\"There is a significant difference between the mean of the observed and expected results (p < 0.05).\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the mean of the observed and expected results (p >= 0.05).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7fd323cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean expected score: 3.6338\n",
      "Mean observed score: 4.7750\n",
      "Mean variance: 1.9486\n",
      "Z-score (mean comparison): 0.8175\n",
      "p-value (mean comparison): 0.2068\n",
      "95% Confidence Interval for Expected Mean: [0.8978, 6.3698]\n",
      "Shapiro-Wilk test statistic: 0.9352\n",
      "Shapiro-Wilk p-value: 0.0238\n",
      "Wilcoxon test statistic: 127.0000\n",
      "Wilcoxon p-value: 0.0001\n",
      "Kolmogorov-Smirnov test statistic: 0.4290\n",
      "Kolmogorov-Smirnov p-value: 0.0000\n",
      "There is no significant difference between the mean of the observed and expected results (p >= 0.05).\n",
      "The data does not follow a normal distribution according to Shapiro-Wilk test (p < 0.05). The Z-score method may not be appropriate.\n",
      "There is a significant difference between the observed and expected results using the Wilcoxon signed-rank test (p < 0.05).\n",
      "The data does not follow a normal distribution according to the Kolmogorov-Smirnov test (p < 0.05). The Z-score method may not be appropriate.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm, shapiro, wilcoxon, kstest\n",
    "\n",
    "# Load CSV files\n",
    "# prob_df = pd.read_csv('DiagnosticProbabilities_8q.csv', index_col=0)\n",
    "# results_df = pd.read_csv('DiagnosticResults_36s_8q.csv', index_col=0)\n",
    "prob_df = pd.read_csv('PreliminaryProbabilities_8q.csv', index_col=0)\n",
    "results_df = pd.read_csv('PreliminaryResults_8q.csv', index_col=0)\n",
    "\n",
    "# Question values\n",
    "# values = np.array([0.110, 0.106, 0.095, 0.174, 0.114, 0.108, 0.096, 0.196])\n",
    "# values = np.array([0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125])\n",
    "values = np.array([1, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "# Calculate the expected score for each student\n",
    "expected_scores = (prob_df * values).sum(axis=1)\n",
    "\n",
    "# Calculate the expected variance for each student\n",
    "expected_variances = ((prob_df * (1 - prob_df)) * (values ** 2)).sum(axis=1)\n",
    "\n",
    "# Calculate the observed score for each student\n",
    "observed_scores = (results_df * values).sum(axis=1)\n",
    "\n",
    "# Calculate the mean expected score and mean variance\n",
    "mean_expected_score = expected_scores.mean()\n",
    "mean_variance = expected_variances.sum() / len(expected_scores)\n",
    "\n",
    "# Calculate the mean observed score\n",
    "mean_observed_score = observed_scores.mean()\n",
    "\n",
    "# Calculate the Z-score for the mean comparison\n",
    "z_score_mean = (mean_observed_score - mean_expected_score) / np.sqrt(mean_variance)\n",
    "\n",
    "# Calculate the p-value for the mean comparison\n",
    "p_value_mean = 1 - norm.cdf(z_score_mean)\n",
    "\n",
    "# Calculate the standard error\n",
    "standard_error = np.sqrt(mean_variance)\n",
    "\n",
    "# Calculate the 95% confidence interval for the expected mean\n",
    "ci_lower = mean_expected_score - 1.96 * standard_error\n",
    "ci_upper = mean_expected_score + 1.96 * standard_error\n",
    "\n",
    "# Perform the Shapiro-Wilk normality test for expected scores\n",
    "shapiro_stat, shapiro_p_value = shapiro(expected_scores)\n",
    "\n",
    "# Perform the Wilcoxon signed-rank test for non-parametric comparison\n",
    "wilcoxon_stat, wilcoxon_p_value = wilcoxon(observed_scores, expected_scores)\n",
    "\n",
    "# Perform the Kolmogorov-Smirnov test for normality\n",
    "ks_stat, ks_p_value = kstest(expected_scores, 'norm', args=(mean_expected_score, np.sqrt(mean_variance)))\n",
    "\n",
    "print(f\"Mean expected score: {mean_expected_score:.4f}\")\n",
    "print(f\"Mean observed score: {mean_observed_score:.4f}\")\n",
    "print(f\"Mean variance: {mean_variance:.4f}\")\n",
    "print(f\"Z-score (mean comparison): {z_score_mean:.4f}\")\n",
    "print(f\"p-value (mean comparison): {p_value_mean:.4f}\")\n",
    "print(f\"95% Confidence Interval for Expected Mean: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "\n",
    "# Shapiro-Wilk test result\n",
    "print(f\"Shapiro-Wilk test statistic: {shapiro_stat:.4f}\")\n",
    "print(f\"Shapiro-Wilk p-value: {shapiro_p_value:.4f}\")\n",
    "\n",
    "# Wilcoxon signed-rank test result\n",
    "print(f\"Wilcoxon test statistic: {wilcoxon_stat:.4f}\")\n",
    "print(f\"Wilcoxon p-value: {wilcoxon_p_value:.4f}\")\n",
    "\n",
    "# Kolmogorov-Smirnov test result\n",
    "print(f\"Kolmogorov-Smirnov test statistic: {ks_stat:.4f}\")\n",
    "print(f\"Kolmogorov-Smirnov p-value: {ks_p_value:.4f}\")\n",
    "\n",
    "# Conclusion\n",
    "if p_value_mean < 0.05:\n",
    "    print(\"There is a significant difference between the mean of the observed and expected results (p < 0.05).\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the mean of the observed and expected results (p >= 0.05).\")\n",
    "\n",
    "if shapiro_p_value < 0.05:\n",
    "    print(\"The data does not follow a normal distribution according to Shapiro-Wilk test (p < 0.05). The Z-score method may not be appropriate.\")\n",
    "else:\n",
    "    print(\"The data follows a normal distribution according to Shapiro-Wilk test (p >= 0.05). The Z-score method is appropriate.\")\n",
    "\n",
    "if wilcoxon_p_value < 0.05:\n",
    "    print(\"There is a significant difference between the observed and expected results using the Wilcoxon signed-rank test (p < 0.05).\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the observed and expected results using the Wilcoxon signed-rank test (p >= 0.05).\")\n",
    "\n",
    "if ks_p_value < 0.05:\n",
    "    print(\"The data does not follow a normal distribution according to the Kolmogorov-Smirnov test (p < 0.05). The Z-score method may not be appropriate.\")\n",
    "else:\n",
    "    print(\"The data follows a normal distribution according to the Kolmogorov-Smirnov test (p >= 0.05). The Z-score method is appropriate.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4562e633",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "### The results in the diagnostic minitest are within the range of expected values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b741661c",
   "metadata": {},
   "source": [
    "# Preliminary minitest (all questions)\n",
    "\n",
    "\n",
    "Here the algorithm to show that the results in the minitest are within the range of the expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c0c60b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean expected score: 0.4426\n",
      "Mean observed score: 0.5867\n",
      "Mean variance: 0.0327\n",
      "Z-score (mean comparison): 0.7974\n",
      "p-value (mean comparison): 0.2126\n",
      "95% Confidence Interval for Expected Mean (Excluding Q3 and Q7): [0.0883, 0.7969]\n",
      "There is no significant difference between the mean of the observed and expected results (p >= 0.05).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Load CSV files\n",
    "prob_df = pd.read_csv('PreliminaryProbabilities_8q.csv', index_col=0)\n",
    "results_df = pd.read_csv('PreliminaryResults_8q.csv', index_col=0)\n",
    "\n",
    "# Question values\n",
    "values = np.array([0.110, 0.106, 0.095, 0.174, 0.114, 0.108, 0.096, 0.196])\n",
    "\n",
    "# Calculate the expected score for each student\n",
    "expected_scores = (prob_df * values).sum(axis=1)\n",
    "\n",
    "# Calculate the expected variance for each student\n",
    "expected_variances = ((prob_df * (1 - prob_df)) * (values ** 2)).sum(axis=1)\n",
    "\n",
    "# Calculate the observed score for each student\n",
    "observed_scores = (results_df * values).sum(axis=1)\n",
    "\n",
    "# Calculate the mean expected score and mean variance\n",
    "mean_expected_score = expected_scores.mean()\n",
    "mean_variance = expected_variances.sum() / len(expected_scores)\n",
    "\n",
    "# Calculate the mean observed score\n",
    "mean_observed_score = observed_scores.mean()\n",
    "\n",
    "# Calculate the Z-score for the mean comparison\n",
    "z_score_mean = (mean_observed_score - mean_expected_score) / np.sqrt(mean_variance)\n",
    "\n",
    "# Calculate the p-value for the mean comparison\n",
    "p_value_mean = 1 - norm.cdf(z_score_mean)\n",
    "\n",
    "# Calculate the standard error\n",
    "standard_error = np.sqrt(mean_variance)\n",
    "\n",
    "# Calculate the 95% confidence interval for the expected mean\n",
    "ci_lower = mean_expected_score - 1.96 * standard_error\n",
    "ci_upper = mean_expected_score + 1.96 * standard_error\n",
    "\n",
    "print(f\"Mean expected score: {mean_expected_score:.4f}\")\n",
    "print(f\"Mean observed score: {mean_observed_score:.4f}\")\n",
    "print(f\"Mean variance: {mean_variance:.4f}\")\n",
    "print(f\"Z-score (mean comparison): {z_score_mean:.4f}\")\n",
    "print(f\"p-value (mean comparison): {p_value_mean:.4f}\")\n",
    "print(f\"95% Confidence Interval for Expected Mean (Excluding Q3 and Q7): [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "\n",
    "# Conclusion\n",
    "if p_value_mean < 0.05:\n",
    "    print(\"There is a significant difference between the mean of the observed and expected results (p < 0.05).\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the mean of the observed and expected results (p >= 0.05).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc98966",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "### The results in the preliminary minitest are within the range of expected values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91926a97",
   "metadata": {},
   "source": [
    "# Final minitest (all questions)\n",
    "\n",
    "Here the algorithm to show that the results in the minitest are within the range of the expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e240acf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean expected score: 0.4426\n",
      "Mean observed score: 0.7977\n",
      "Mean variance: 0.0327\n",
      "Z-score (mean comparison): 1.9642\n",
      "p-value (mean comparison): 0.0248\n",
      "95% Confidence Interval for Expected Mean (Excluding Q3 and Q7): [0.0883, 0.7969]\n",
      "There is a significant difference between the mean of the observed and expected results (p < 0.05).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Load CSV files\n",
    "prob_df = pd.read_csv('PreliminaryProbabilities_8q.csv', index_col=0)\n",
    "results_df = pd.read_csv('FinalResults_8q.csv', index_col=0)\n",
    "\n",
    "# Question values\n",
    "values = np.array([0.110, 0.106, 0.095, 0.174, 0.114, 0.108, 0.096, 0.196])\n",
    "\n",
    "# Calculate the expected score for each student\n",
    "# Calculate the expected variance for each student\n",
    "expected_variances = ((prob_df * (1 - prob_df)) * (values ** 2)).sum(axis=1)\n",
    "\n",
    "# Calculate the observed score for each student\n",
    "observed_scores = (results_df * values).sum(axis=1)\n",
    "\n",
    "# Calculate the mean expected score and mean variance\n",
    "mean_expected_score = expected_scores.mean()\n",
    "mean_variance = expected_variances.sum() / len(expected_scores)\n",
    "\n",
    "# Calculate the mean observed score\n",
    "mean_observed_score = observed_scores.mean()\n",
    "\n",
    "# Calculate the Z-score for the mean comparison\n",
    "z_score_mean = (mean_observed_score - mean_expected_score) / np.sqrt(mean_variance)\n",
    "\n",
    "# Calculate the p-value for the mean comparison\n",
    "p_value_mean = 1 - norm.cdf(z_score_mean)\n",
    "\n",
    "# Calculate the standard error\n",
    "standard_error = np.sqrt(mean_variance)\n",
    "\n",
    "# Calculate the 95% confidence interval for the expected mean\n",
    "ci_lower = mean_expected_score - 1.96 * standard_error\n",
    "ci_upper = mean_expected_score + 1.96 * standard_error\n",
    "\n",
    "print(f\"Mean expected score: {mean_expected_score:.4f}\")\n",
    "print(f\"Mean observed score: {mean_observed_score:.4f}\")\n",
    "print(f\"Mean variance: {mean_variance:.4f}\")\n",
    "print(f\"Z-score (mean comparison): {z_score_mean:.4f}\")\n",
    "print(f\"p-value (mean comparison): {p_value_mean:.4f}\")\n",
    "print(f\"95% Confidence Interval for Expected Mean (Excluding Q3 and Q7): [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "\n",
    "# Conclusion\n",
    "if p_value_mean < 0.05:\n",
    "    print(\"There is a significant difference between the mean of the observed and expected results (p < 0.05).\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the mean of the observed and expected results (p >= 0.05).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf1fccc",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "### The results in the final minitest are not within the range of expected values. The results are too high, even though questions 3 and 7 were only answered correctly 6 and 5 times respectively out of 40 attempts each.\n",
    "\n",
    "\n",
    "The problem with this approach is that this is not an optimal measure to compare with the other two cases. Whereas for 1, 2, 4, 5, 6 and 8, students are being influenced towards correctness, for questions 3 and 7 students were swayed towarsd incorrectness. The optimal measure is one that combines how much the students deviated from expectations in both directions, instead of how much they deviated to correctness despite the results in Q3 an 7, which is the current case.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71304544",
   "metadata": {},
   "source": [
    "# Final minitest (non-critical questions 1,2,4,5,6,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "87577a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean expected score (Excluding Q3 and Q7): 0.3368\n",
      "Mean observed score (Excluding Q3 and Q7): 0.7714\n",
      "Mean variance (Excluding Q3 and Q7): 0.0282\n",
      "Z-score (mean comparison, Excluding Q3 and Q7): 2.5879\n",
      "p-value (mean comparison, Excluding Q3 and Q7): 0.0048\n",
      "95% Confidence Interval for Expected Mean (Excluding Q3 and Q7): [0.0077, 0.6660]\n",
      "There is a significant difference between the mean of the observed and expected results (excluding Q3 and Q7) (p < 0.05).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Load CSV files\n",
    "prob_df = pd.read_csv('PreliminaryProbabilities_8q.csv', index_col=0)\n",
    "results_df = pd.read_csv('FinalResults_8q.csv', index_col=0)\n",
    "\n",
    "# Question values, excluding questions 3 and 7 (indexes 2 and 6)\n",
    "values = np.array([0.110, 0.106, 0.174, 0.114, 0.108, 0.196])  # Excluding Q3 and Q7\n",
    "\n",
    "# Drop columns for questions 3 and 7 (indexes 2 and 6)\n",
    "prob_subset = prob_df.drop(columns=[prob_df.columns[2], prob_df.columns[6]])\n",
    "results_subset = results_df.drop(columns=[results_df.columns[2], results_df.columns[6]])\n",
    "\n",
    "# Calculate the expected score for each student\n",
    "expected_scores = (prob_subset * values).sum(axis=1)\n",
    "\n",
    "# Calculate the expected variance for each student\n",
    "expected_variances = ((prob_subset * (1 - prob_subset)) * (values ** 2)).sum(axis=1)\n",
    "\n",
    "# Calculate the observed score for each student\n",
    "observed_scores = (results_subset * values).sum(axis=1)\n",
    "\n",
    "# Calculate the mean expected score and mean variance\n",
    "mean_expected_score = expected_scores.mean()\n",
    "mean_variance = expected_variances.sum() / len(expected_scores)\n",
    "\n",
    "# Calculate the mean observed score\n",
    "mean_observed_score = observed_scores.mean()\n",
    "\n",
    "# Calculate the Z-score for the mean comparison\n",
    "z_score_mean = (mean_observed_score - mean_expected_score) / np.sqrt(mean_variance)\n",
    "\n",
    "# Calculate the p-value for the mean comparison\n",
    "p_value_mean = 1 - norm.cdf(z_score_mean)\n",
    "\n",
    "# Calculate the standard error\n",
    "standard_error = np.sqrt(mean_variance)\n",
    "\n",
    "# Calculate the 95% confidence interval for the expected mean\n",
    "ci_lower = mean_expected_score - 1.96 * standard_error\n",
    "ci_upper = mean_expected_score + 1.96 * standard_error\n",
    "\n",
    "\n",
    "print(f\"Mean expected score (Excluding Q3 and Q7): {mean_expected_score:.4f}\")\n",
    "print(f\"Mean observed score (Excluding Q3 and Q7): {mean_observed_score:.4f}\")\n",
    "print(f\"Mean variance (Excluding Q3 and Q7): {mean_variance:.4f}\")\n",
    "print(f\"Z-score (mean comparison, Excluding Q3 and Q7): {z_score_mean:.4f}\")\n",
    "print(f\"p-value (mean comparison, Excluding Q3 and Q7): {p_value_mean:.4f}\")\n",
    "print(f\"95% Confidence Interval for Expected Mean (Excluding Q3 and Q7): [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "\n",
    "# Conclusion\n",
    "if p_value_mean < 0.05:\n",
    "    print(\"There is a significant difference between the mean of the observed and expected results (excluding Q3 and Q7) (p < 0.05).\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the mean of the observed and expected results (excluding Q3 and Q7) (p >= 0.05).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119eecc5",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "### The results in the non-critical questions of the final minitest are not within the range of expected values. The results are too high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8faa67",
   "metadata": {},
   "source": [
    "# Final minitest (critical questions 3 & 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bb8cd79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean expected score (Only Q3 and Q7): 0.1058\n",
      "Mean observed score (Only Q3 and Q7): 0.0263\n",
      "Mean variance (Only Q3 and Q7): 0.0045\n",
      "Z-score (mean comparison, Only Q3 and Q7): -1.1892\n",
      "p-value (mean comparison, Only Q3 and Q7): 0.8828\n",
      "95% Confidence Interval for Expected Mean (Only Q3 and Q7): [-0.0253, 0.2369]\n",
      "There is no significant difference between the mean of the observed and expected results (Only Q3 and Q7) (p >= 0.05).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Load CSV files\n",
    "prob_df = pd.read_csv('PreliminaryProbabilities_8q.csv', index_col=0)\n",
    "results_df = pd.read_csv('FinalResults_8q.csv', index_col=0)\n",
    "\n",
    "# Question values, including only questions 3 and 7 (indexes 2 and 6)\n",
    "values = np.array([0.095, 0.096])  # Q3 and Q7\n",
    "\n",
    "# Drop columns for questions 3 and 7 (indexes 2 and 6)\n",
    "prob_subset = prob_df.drop(columns=[prob_df.columns[0], prob_df.columns[1], prob_df.columns[3], prob_df.columns[4], prob_df.columns[5], prob_df.columns[7], ])\n",
    "results_subset = results_df.drop(columns=[results_df.columns[0], results_df.columns[1], results_df.columns[3], results_df.columns[4], results_df.columns[5], results_df.columns[7]])\n",
    "\n",
    "# Calculate the expected score for each student\n",
    "expected_scores = (prob_subset * values).sum(axis=1)\n",
    "\n",
    "# Calculate the expected variance for each student\n",
    "expected_variances = ((prob_subset * (1 - prob_subset)) * (values ** 2)).sum(axis=1)\n",
    "\n",
    "# Calculate the observed score for each student\n",
    "observed_scores = (results_subset * values).sum(axis=1)\n",
    "\n",
    "# Calculate the mean expected score and mean variance\n",
    "mean_expected_score = expected_scores.mean()\n",
    "mean_variance = expected_variances.sum() / len(expected_scores)\n",
    "\n",
    "# Calculate the mean observed score\n",
    "mean_observed_score = observed_scores.mean()\n",
    "\n",
    "# Calculate the Z-score for the mean comparison\n",
    "z_score_mean = (mean_observed_score - mean_expected_score) / np.sqrt(mean_variance)\n",
    "\n",
    "# Calculate the p-value for the mean comparison\n",
    "p_value_mean = 1 - norm.cdf(z_score_mean)\n",
    "\n",
    "# Calculate the standard error\n",
    "standard_error = np.sqrt(mean_variance)\n",
    "\n",
    "# Calculate the 95% confidence interval for the expected mean\n",
    "ci_lower = mean_expected_score - 1.96 * standard_error\n",
    "ci_upper = mean_expected_score + 1.96 * standard_error\n",
    "\n",
    "\n",
    "print(f\"Mean expected score (Only Q3 and Q7): {mean_expected_score:.4f}\")\n",
    "print(f\"Mean observed score (Only Q3 and Q7): {mean_observed_score:.4f}\")\n",
    "print(f\"Mean variance (Only Q3 and Q7): {mean_variance:.4f}\")\n",
    "print(f\"Z-score (mean comparison, Only Q3 and Q7): {z_score_mean:.4f}\")\n",
    "print(f\"p-value (mean comparison, Only Q3 and Q7): {p_value_mean:.4f}\")\n",
    "print(f\"95% Confidence Interval for Expected Mean (Only Q3 and Q7): [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "\n",
    "# Conclusion\n",
    "if p_value_mean < 0.05:\n",
    "    print(\"There is a significant difference between the mean of the observed and expected results (Only Q3 and Q7) (p < 0.05).\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the mean of the observed and expected results (Only Q3 and Q7) (p >= 0.05).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b16e50",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "### This Approach is suitable for the Overall Minitest or 6 questions, but not for Questions 3 and 7\n",
    "\n",
    "Use of Z-Score for the Entire Minitest:\n",
    "For the entire minitest, with 8 questions and 40 students, or an extract of 6 questions and 40 students there is a larger dataset (320 and 240 data points respectively). These sample sizes are sufficient to assume a normal distribution due to the Central Limit Theorem, making the Z-score and confidence interval approach a valid method to check if the overall results are within the expected range.\n",
    "\n",
    "Not Using Z-Score for Questions 3 and 7:\n",
    "For questions 3 and 7, the dataset is much smaller (only 2 questions and 40 students, resulting in 80 data points). With such a small dataset and limited variation, the normality assumption is not reliable, and the Z-score approach is not sensitive enough to detect the anomaly you expect.\n",
    "In such cases, a different statistical test, like the binomial test, is more appropriate because it directly assesses whether the number of successes (correct answers) significantly deviates from the expected probability for a binomial distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "76cd64c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct answers for Q3: 6\n",
      "Expected probability for Q3: 0.5333\n",
      "p-value for Q3 (binomial test): 0.0000\n",
      "Total correct answers for Q7: 5\n",
      "Expected probability for Q7: 0.5743\n",
      "p-value for Q7 (binomial test): 0.0000\n",
      "There is a significant difference for Q3 (p < 0.05). The observed number of correct answers is significantly lower than expected.\n",
      "There is a significant difference for Q7 (p < 0.05). The observed number of correct answers is significantly lower than expected.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import binomtest\n",
    "\n",
    "# Load CSV files\n",
    "prob_df = pd.read_csv('PreliminaryProbabilities_8q.csv', index_col=0)\n",
    "results_df = pd.read_csv('FinalResults_8q.csv', index_col=0)\n",
    "\n",
    "# Select only columns for questions 3 and 7 (indexes 2 and 6)\n",
    "prob_q3_q7 = prob_df.iloc[:, [2, 6]]\n",
    "results_q3_q7 = results_df.iloc[:, [2, 6]]\n",
    "\n",
    "# Calculate the total number of correct answers for each question\n",
    "correct_answers_q3 = results_q3_q7.iloc[:, 0].sum()  # Total correct answers for Q3\n",
    "correct_answers_q7 = results_q3_q7.iloc[:, 1].sum()  # Total correct answers for Q7\n",
    "\n",
    "# Number of students (total trials)\n",
    "n_students = len(results_q3_q7)\n",
    "\n",
    "# Calculate the mean expected probability of correctness for each question\n",
    "p_q3 = prob_q3_q7.iloc[:, 0].mean()\n",
    "p_q7 = prob_q3_q7.iloc[:, 1].mean()\n",
    "\n",
    "# Perform the binomial test for each question\n",
    "p_value_q3 = binomtest(correct_answers_q3, n_students, p_q3, alternative='less').pvalue\n",
    "p_value_q7 = binomtest(correct_answers_q7, n_students, p_q7, alternative='less').pvalue\n",
    "\n",
    "print(f\"Total correct answers for Q3: {correct_answers_q3}\")\n",
    "print(f\"Expected probability for Q3: {p_q3:.4f}\")\n",
    "print(f\"p-value for Q3 (binomial test): {p_value_q3:.4f}\")\n",
    "\n",
    "print(f\"Total correct answers for Q7: {correct_answers_q7}\")\n",
    "print(f\"Expected probability for Q7: {p_q7:.4f}\")\n",
    "print(f\"p-value for Q7 (binomial test): {p_value_q7:.4f}\")\n",
    "\n",
    "# Conclusion\n",
    "if p_value_q3 < 0.05:\n",
    "    print(\"There is a significant difference for Q3 (p < 0.05). The observed number of correct answers is significantly lower than expected.\")\n",
    "else:\n",
    "    print(\"There is no significant difference for Q3 (p >= 0.05).\")\n",
    "\n",
    "if p_value_q7 < 0.05:\n",
    "    print(\"There is a significant difference for Q7 (p < 0.05). The observed number of correct answers is significantly lower than expected.\")\n",
    "else:\n",
    "    print(\"There is no significant difference for Q7 (p >= 0.05).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319cf5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
